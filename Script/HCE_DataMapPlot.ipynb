{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datamapplot -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install pyarrow -q\n",
        "!pip install sentence_transformers -q\n",
        "!pip install numpy==1.26.4 -q\n",
        "!pip install pandas -q\n",
        "!pip install --upgrade scipy\n",
        "!pip install nltk -q"
      ],
      "metadata": {
        "id": "7_l4YG3mCjve"
      },
      "id": "7_l4YG3mCjve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datamapplot\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.colors as mcolors\n",
        "import re\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time"
      ],
      "metadata": {
        "id": "wIdTZh-7Fk1n"
      },
      "id": "wIdTZh-7Fk1n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Generating Sentence Embeddings with Sentence Transformers\n",
        "\n",
        "This script uses the `SentenceTransformers` library to generate sentence embeddings from any type of text. Here, we apply it to interview transcripts, enabling us to analyze how interviews relate based on their content. The text is transformed into numerical embeddings that capture semantic meaning, making them ideal for tasks like similarity detection or data clustering.\n",
        "\n",
        "In this script, the default `SentenceTransformer` model is **paraphrase-multilingual-MiniLM-L12-v2**, which is trained on many different languages. However, you can easily change it to suit your needs. Here is a list of different [pre-trained models](https://sbert.net/docs/sentence_transformer/pretrained_models.html).\n"
      ],
      "metadata": {
        "id": "E3ZUwxjF2iuN"
      },
      "id": "E3ZUwxjF2iuN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Load the CSV OR XLSX file\n",
        "df = pd.read_csv('Text-File.csv') # Your csv file name\n",
        "#df = pd.read_execel ('TEXT.xlsx')\n",
        "\n",
        "# Step 2: Prepare the text data\n",
        "text_data = df['Text'].tolist()\n",
        "\n",
        "# Step 3: Generate embeddings with progress bar and timer\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2') ## English model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') ## Multilingual model\n",
        "# Step 3: Generate embeddings\n",
        "start_time = time.time()\n",
        "\n",
        "embeddings = model.encode(text_data, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Embeddings generated in {elapsed_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "6hITvg3r5AHD"
      },
      "id": "6hITvg3r5AHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Visualizing Sentence Embeddings with DataMapPlot\n",
        "\n",
        "Once the sentence embeddings are generated, the next step is to visualize them. This script uses [DataMapPlot](https://datamapplot.readthedocs.io/en/latest/) to create an interactive plot, showing how the embeddings are clustered and organized in a 2D space.\n",
        "\n",
        "### Visualization 1: DataMapPlot without Labels\n",
        "\n",
        "This first visualization is created without any labels. It shows how different clusters are formed based on the semantic similarity between interviews. The goal here is to explore the clusters and start identifying patterns in the data, without the influence of labels. Once we have a clear view of these clusters, we can move on to annotating them for further analysis."
      ],
      "metadata": {
        "id": "KOPrdCyk2pd2"
      },
      "id": "KOPrdCyk2pd2"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure embeddings are in NumPy array format\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Number of clusters\n",
        "n_clusters = 10  # You can adjust this as needed\n",
        "\n",
        "# Step 1: Create a data map using t-SNE\n",
        "n_samples = embeddings.shape[0]\n",
        "perplexity = min(30, (n_samples - 1) // 3)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "data_map = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Step 2: Perform KMeans clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels_int = kmeans.fit_predict(data_map)\n",
        "\n",
        "# Assign generic cluster names\n",
        "label_topic_map = {label: f\"Cluster {label + 1}\" for label in range(n_clusters)}\n",
        "labels_topic = np.array([label_topic_map.get(label, \"Unknown\") for label in labels_int])\n",
        "\n",
        "# Step 3: Prepare hover text\n",
        "hover_text = df['Text'].astype(str).tolist()\n",
        "\n",
        "# Step 4: Create a color palette\n",
        "color_palette = list(mcolors.TABLEAU_COLORS.values())\n",
        "\n",
        "# Step 5: Generate marker colors\n",
        "unique_labels = np.unique(labels_topic)\n",
        "color_mapping = {label: color_palette[i % len(color_palette)] for i, label in enumerate(unique_labels)}\n",
        "marker_color_array = [color_mapping[label] for label in labels_topic]\n",
        "\n",
        "# Step 6: Set marker sizes\n",
        "marker_size_array = df['Text'].str.len().values.astype(np.float32)\n",
        "min_size, max_size = 5, 15\n",
        "# Normalize marker sizes between min_size and max_size\n",
        "if marker_size_array.max() != marker_size_array.min():\n",
        "    marker_size_array = min_size + (max_size - min_size) * (\n",
        "        (marker_size_array - marker_size_array.min()) / (marker_size_array.max() - marker_size_array.min())\n",
        "    )\n",
        "else:\n",
        "    marker_size_array = np.full_like(marker_size_array, (min_size + max_size) / 2)\n",
        "\n",
        "# Step 7: Set point radius min and max pixels\n",
        "point_radius_min_pixels = 2\n",
        "point_radius_max_pixels = 10\n",
        "\n",
        "# Create the interactive plot\n",
        "try:\n",
        "    plot = datamapplot.create_interactive_plot(\n",
        "        data_map,\n",
        "        labels_topic,  # Use the generic cluster names\n",
        "        hover_text=hover_text,\n",
        "        font_family=\"Merriweather\",\n",
        "        title=\"Interviews\",\n",
        "        sub_title=\"Interactive plot with Generic Cluster Names\",\n",
        "        enable_search=True,\n",
        "        darkmode=True,\n",
        "        marker_color_array=marker_color_array,\n",
        "        marker_size_array=marker_size_array,\n",
        "        point_radius_min_pixels=point_radius_min_pixels,\n",
        "        point_radius_max_pixels=point_radius_max_pixels,\n",
        "        point_line_width=0,\n",
        "        cluster_boundary_polygons=False,  # Disable if not needed\n",
        "        cluster_boundary_line_width=2,\n",
        "    )\n",
        "\n",
        "    # Save the plot to an HTML file\n",
        "    plot.save(\"Interviews: Clusters_Generic_Names.html\")\n",
        "    print(\"Plot with generic cluster names saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or saving the plot: {e}\")\n"
      ],
      "metadata": {
        "id": "7KHNclTD5E7F"
      },
      "id": "7KHNclTD5E7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization 2: DataMapPlot with User Annotation Labels"
      ],
      "metadata": {
        "id": "2ebnAC-M26Tg"
      },
      "id": "2ebnAC-M26Tg"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_user_defined_names(n_clusters):\n",
        "    \"\"\"\n",
        "    Prompt user to input names for each cluster without any suggestions.\n",
        "    \"\"\"\n",
        "    print(f\"\\nPlease provide names for each of the {n_clusters} clusters.\")\n",
        "    cluster_names = []\n",
        "    for i in range(n_clusters):a\n",
        "        user_input = input(f\"Enter name for Cluster {i + 1}: \").strip()\n",
        "        if user_input:\n",
        "            cluster_names.append(user_input)\n",
        "        else:\n",
        "            cluster_names.append(f\"Cluster {i + 1}\")\n",
        "    return cluster_names\n",
        "\n",
        "def save_cluster_names(cluster_names, filename=\"cluster_names.json\"):\n",
        "    \"\"\"\n",
        "    Save the cluster names to a JSON file.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(cluster_names, f, indent=4)\n",
        "    print(f\"\\nCluster names saved to {filename}.\")\n",
        "\n",
        "# Ensure embeddings are in NumPy array format\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Number of clusters\n",
        "n_clusters = 10  # Adjust as needed\n",
        "\n",
        "# Step 1: Create a data map using t-SNE\n",
        "n_samples = embeddings.shape[0]\n",
        "perplexity = min(30, (n_samples - 1) // 3)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "data_map = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Step 2: Perform KMeans clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "labels_int = kmeans.fit_predict(data_map)\n",
        "\n",
        "# Step 3: Get user-defined cluster names without suggestions\n",
        "cluster_names = get_user_defined_names(n_clusters)\n",
        "\n",
        "label_topic_map = {label: cluster_names[label] for label in range(n_clusters)}\n",
        "labels_topic = np.array([label_topic_map.get(label, \"Unknown\") for label in labels_int])\n",
        "\n",
        "# Step 4: Prepare hover text\n",
        "hover_text = df['Text'].astype(str).tolist()\n",
        "\n",
        "# Step 5: Create a color palette\n",
        "color_palette = list(mcolors.TABLEAU_COLORS.values())\n",
        "\n",
        "# Step 6: Generate marker colors\n",
        "unique_labels = np.unique(labels_topic)\n",
        "color_mapping = {label: color_palette[i % len(color_palette)] for i, label in enumerate(unique_labels)}\n",
        "marker_color_array = [color_mapping[label] for label in labels_topic]\n",
        "\n",
        "# Step 7: Set marker sizes\n",
        "marker_size_array = df['Text'].str.len().values.astype(np.float32)\n",
        "min_size, max_size = 5, 15\n",
        "# Normalize marker sizes between min_size and max_size\n",
        "if marker_size_array.max() != marker_size_array.min():\n",
        "    marker_size_array = min_size + (max_size - min_size) * (\n",
        "        (marker_size_array - marker_size_array.min()) / (marker_size_array.max() - marker_size_array.min())\n",
        "    )\n",
        "else:\n",
        "    marker_size_array = np.full_like(marker_size_array, (min_size + max_size) / 2)\n",
        "\n",
        "# Step 8: Set point radius min and max pixels\n",
        "point_radius_min_pixels = 2\n",
        "point_radius_max_pixels = 10\n",
        "\n",
        "# Step 9: Create the interactive plot\n",
        "try:\n",
        "    plot = datamapplot.create_interactive_plot(\n",
        "        data_map,\n",
        "        labels_topic,  # Use the user-defined cluster names\n",
        "        hover_text=hover_text,\n",
        "        font_family=\"Merriweather\",\n",
        "        title=\"Interview Data Map\",\n",
        "        sub_title=\"Interactive plot with User-Defined Cluster Names\",\n",
        "        enable_search=True,\n",
        "        darkmode=True,\n",
        "        marker_color_array=marker_color_array,\n",
        "        marker_size_array=marker_size_array,\n",
        "        point_radius_min_pixels=point_radius_min_pixels,\n",
        "        point_radius_max_pixels=point_radius_max_pixels,\n",
        "        point_line_width=0,\n",
        "        cluster_boundary_polygons=False,  # Disable if not needed\n",
        "        cluster_boundary_line_width=2,\n",
        "    )\n",
        "\n",
        "    # Save the plot to an HTML file\n",
        "    plot_filename = \"Clusters_User_Defined_Names.html\"\n",
        "    plot.save(plot_filename)\n",
        "    print(f\"Plot with user-defined cluster names saved successfully as {plot_filename}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or saving the plot: {e}\")\n"
      ],
      "metadata": {
        "id": "DMuNWIlo5Koa"
      },
      "id": "DMuNWIlo5Koa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization 3: Automating Labels of Clusters (TF-IDF)\n",
        "\n",
        "In this example, we demonstrate how machine learning can be used to automatically label the different clusters. Here, we use TF-IDF (Term Frequency-Inverse Document Frequency), a statistical method that evaluates how frequently a word appears in a document versus how commonly it appears across all documents. This is one of the simplest ways to name clusters, making it efficient for use without requiring significant computational power.\n",
        "\n",
        "For more advanced methods, you could consider using [BERTopic](https://maartengr.github.io/BERTopic/index.html), or even combining BERTopic with an open-source generative language model, which can be run through tools like [Ollama](https://ollama.com/), to generate more nuanced automated cluster labels.\n"
      ],
      "metadata": {
        "id": "T5YTGiK23ByI"
      },
      "id": "T5YTGiK23ByI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba6e794-a7e9-410f-9af8-9065ade4043c",
      "metadata": {
        "id": "7ba6e794-a7e9-410f-9af8-9065ade4043c"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Define stop words for different languages\n",
        "stop_words = {\n",
        "    'english': stopwords.words('english'),\n",
        "    'danish': stopwords.words('danish'),\n",
        "    'german': stopwords.words('german')\n",
        "}\n",
        "\n",
        "def get_cluster_topic(cluster_texts, language='english', n_terms=5):\n",
        "    cluster_texts = [text for text in cluster_texts if isinstance(text, str) and text.strip()]\n",
        "    if not cluster_texts:\n",
        "        return []\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=stop_words[language],\n",
        "        max_features=1000,\n",
        "        ngram_range=(2, 3)  # Use bigrams and trigrams\n",
        "    )\n",
        "    try:\n",
        "        X = vectorizer.fit_transform(cluster_texts)\n",
        "        if X.shape[1] == 0:\n",
        "            return []\n",
        "\n",
        "        tf_idf_sum = X.sum(axis=0).A1  # Sum TF-IDF scores across all documents\n",
        "        terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "        top_indices = tf_idf_sum.argsort()[::-1]\n",
        "        top_terms = [terms[i] for i in top_indices]\n",
        "\n",
        "        return top_terms\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_cluster_topic: {e}\")\n",
        "        return []\n",
        "\n",
        "# Ensure embeddings are in NumPy array format\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Number of samples\n",
        "n_samples = embeddings.shape[0]\n",
        "\n",
        "# Adjust perplexity based on the number of samples\n",
        "perplexity = min(30, (n_samples - 1) // 3)\n",
        "\n",
        "# Step 1: Create a data map using t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "data_map = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Step 2: Perform hierarchical clustering\n",
        "n_clusters_list = [2, 5, 10]  # Adjust these numbers for your desired hierarchy levels\n",
        "labels_layers = []\n",
        "\n",
        "for n_clusters in n_clusters_list:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels_int = kmeans.fit_predict(data_map)\n",
        "\n",
        "    used_topics = set()\n",
        "\n",
        "    # Generate topic names for each cluster\n",
        "    label_topic_map = {}\n",
        "    for label in range(n_clusters):\n",
        "        indices = np.where(labels_int == label)[0]\n",
        "        if len(indices) == 0:\n",
        "            label_topic_map[label] = f\"{label}: No data\"\n",
        "            continue\n",
        "        cluster_texts = df['Text'].iloc[indices].astype(str).tolist()\n",
        "        top_terms = get_cluster_topic(cluster_texts, language='english', n_terms=5)\n",
        "\n",
        "        # Select the first unused term as the topic name\n",
        "        topic_name = None\n",
        "        for term in top_terms:\n",
        "            if term not in used_topics:\n",
        "                topic_name = term\n",
        "                used_topics.add(term)\n",
        "                break\n",
        "\n",
        "        if topic_name is None:\n",
        "            # All terms have been used; default to the highest scoring term with cluster label\n",
        "            topic_name = f\"{top_terms[0]} {label}\" if top_terms else f\"Cluster {label}\"\n",
        "\n",
        "        label_topic_map[label] = f\"{label}: {topic_name}\"\n",
        "\n",
        "    # Convert integer labels to topic names\n",
        "    labels_topic = np.array([label_topic_map.get(label, f\"{label}: Unknown\") for label in labels_int])\n",
        "    labels_layers.append(labels_topic)\n",
        "\n",
        "# Step 3: Prepare hover text\n",
        "hover_text = df['Text'].astype(str).tolist()\n",
        "\n",
        "# Step 4: Create a color palette\n",
        "color_palette = list(mcolors.TABLEAU_COLORS.values())\n",
        "\n",
        "# Step 5: Generate marker colors using the last layer of labels\n",
        "labels = labels_layers[-1]  # Use the last layer for coloring\n",
        "\n",
        "# Create a color mapping\n",
        "unique_labels = np.unique(labels)\n",
        "color_mapping = {label: color_palette[i % len(color_palette)] for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Generate marker colors\n",
        "marker_color_array = [color_mapping[label] for label in labels]\n",
        "\n",
        "# Step 6: Set marker sizes\n",
        "marker_size_array = df['Text'].str.len().values.astype(np.float32)\n",
        "min_size, max_size = 5, 15\n",
        "# Normalize marker sizes between min_size and max_size\n",
        "if marker_size_array.max() != marker_size_array.min():\n",
        "    marker_size_array = min_size + (max_size - min_size) * (\n",
        "        (marker_size_array - marker_size_array.min()) / (marker_size_array.max() - marker_size_array.min())\n",
        "    )\n",
        "else:\n",
        "    marker_size_array = np.full_like(marker_size_array, (min_size + max_size) / 2)\n",
        "\n",
        "# Step 7: Set point radius min and max pixels\n",
        "point_radius_min_pixels = 2\n",
        "point_radius_max_pixels = 10\n",
        "\n",
        "# Create the interactive plot\n",
        "try:\n",
        "    plot = datamapplot.create_interactive_plot(\n",
        "        data_map,\n",
        "        *labels_layers,  # Use the labels with topic names\n",
        "        hover_text=hover_text,\n",
        "        font_family=\"Merriweather\",\n",
        "        title=\"Interviews\",\n",
        "        sub_title=\"Interactive plot of Interviews\",\n",
        "        enable_search=True,\n",
        "        darkmode=True,\n",
        "        marker_color_array=marker_color_array,\n",
        "        marker_size_array=marker_size_array,\n",
        "        point_radius_min_pixels=point_radius_min_pixels,\n",
        "        point_radius_max_pixels=point_radius_max_pixels,\n",
        "        point_line_width=0,\n",
        "        cluster_boundary_polygons=False,\n",
        "        cluster_boundary_line_width=2,\n",
        "    )\n",
        "\n",
        "    # Save the plot to an HTML file\n",
        "    plot.save(\"Interviews_TF_IDF.html\")\n",
        "    print(\"Plot saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or displaying the plot: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 ",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}